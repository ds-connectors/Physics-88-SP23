{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 5: Fitting (cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete this homework assignment in code cells in the iPython notebook. Include comments in your code when necessary.  Please rename the notebook as SIS ID_HW05.ipynb (your student ID number) and save the notebook once you have executed it as a PDF  (note, that when saving as PDF you don't want to use the option with latex because it crashes, but rather the one to save it directly as a PDF). \n",
    "\n",
    "**The homework should be submitted on bCourses under the Assignments tab (both the .ipynb and .pdf files). Please label it by your student ID number (SIS ID)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Optical Pumping experiment\n",
    "\n",
    "One of the experiments in the 111B (111-ADV) lab is the study of the optical pumping of atomic rubidium. In that experiment, we measure the resonant frequency of a Zeeman transition as a function of the applied current (local magnetic field). Consider a mock data set:\n",
    "<table border=\"1\" align=\"center\">\n",
    "\n",
    "<tr>\n",
    "<td>Current <i>I</i> (Amps)\n",
    "</td><td>0.0 </td><td> 0.2 </td><td> 0.4 </td><td> 0.6 </td><td> 0.8 </td><td> 1.0 </td><td> 1.2 </td><td> 1.4 </td><td> 1.6 </td><td> 1.8 </td><td> 2.0 </td><td> 2.2\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td>Frequency <i>f</i> (MHz)\n",
    "</td><td> 0.14 </td><td> 0.60 </td><td> 1.21 </td><td> 1.74 </td><td> 2.47 </td><td> 3.07 </td><td> 3.83 </td><td> 4.16 </td><td> 5.08 </td><td> 5.40 </td><td> 6.31 </td><td> 6.78\n",
    "</td></tr></table>\n",
    "\n",
    "1. Plot a graph of the pairs of values. Assuming a linear relationship between $I$ and $f$, determine the slope and the intercept of the best-fit line using the least-squares method with equal weights, and draw the best-fit line through the data points in the graph.\n",
    "1. From what s/he knows about the equipment used to measure the resonant frequency, your lab partner hastily estimates the uncertainty in the measurement of $f$ to be $\\sigma(f) = 0.01$ MHz. Estimate the probability that the straight line you found is an adequate description of the observed data if it is distributed with the uncertainty guessed by your lab partner. (Hint: use scipy.stats.chi2 class to compute the quantile of the chi2 distribution).  What can you conclude from these results? \n",
    "1. Repeat the analysis assuming your partner estimated the uncertainty to be $\\sigma(f) = 1$ MHz. What can you conclude from these results?\n",
    "1. Assume that the best-fit line found in Part 1 is a good fit to the data. Estimate the uncertainty in measurement of $y$ from the scatter of the observed data about this line. Again, assume that all the data points have equal weight. Use this to estimate the uncertainty in both the slope and the intercept of the best-fit line. This is the technique you will use in the Optical Pumping lab to determine the uncertainties in the fit parameters.\n",
    "1. Now assume that the uncertainty in each value of $f$ grows with $f$: $\\sigma(f) = 0.10 + 0.03 * f$ (MHz). Determine the slope and the intercept of the best-fit line using the least-squares method with unequal weights (weighted least-squares fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import *\n",
    "import scipy.stats\n",
    "import scipy.optimize as fitter\n",
    "\n",
    "# Use current as the x-variable in your plots/fitting\n",
    "current = np.arange(0, 2.3, .2)  # Amps\n",
    "f = np.array([.14, .6, 1.21, 1.74, 2.47, 3.07, 3.83, 4.16, 5.08, 5.4, 6.31, 6.78]) # MHz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x, slope, intercept):\n",
    "    '''Model function to use with curve_fit();\n",
    "       it should take the form of a line'''\n",
    "    \n",
    "# Use fitter.curve_fit() to get the line of best fit\n",
    "# Plot this line, along with the data points -- remember to label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty short, but the statistics might be a bit complicated.  Ask questions if you need advice or help.  Next, the problem is basically asking you to compute the $\\chi^2$ for the above fit twice, once with $0.01$ as the error for each point (in the 'denominator' of the $\\chi^2$ formula) and once with $1$.  \n",
    "\n",
    "These values can then be compared to a \"range of acceptable $\\chi^2$ values\", found with `scipy.stats.chi2.ppf()` -- which takes two inputs.  The second input should be the number of degrees of freedom used during fitting (# data points minus the 2 free parameters).  The first input should be something like $0.05$ and $0.95$ (one function call of `scipy.stats.chi2.ppf()` for each endpoint fo the acceptable range).  If the calculated $\\chi^2$ statistic falls within this range, then the assumed uncertainty is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, estimate the uncertainty in the frequency measurements, and use this to find the uncertainty in the best-fit parameters.  [This document](https://pages.mtu.edu/~fmorriso/cm3215/UncertaintySlopeInterceptOfLeastSquaresFit.pdf) is a good resource for learning to propagate errors in the context of linear fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, repeat the fitting with the weighted errors (from the $\\sigma(f)$ uncertainty formula) given to `scipy.optimize.curve_fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Gamma-ray peak\n",
    "\n",
    "[Some of you may recognize this problem from Advanced Lab's Error Analysis Exercise. That's not an accident. You may also recognize this dataset from Homework04. That's not an accident either.]\n",
    "\n",
    "You are given a dataset (`peak.dat`) from a gamma-ray experiment consisting of ~1000 events. Each line in the file corresponds to one recorded gamma-ray event, and stores the the measured energy of the gamma-ray (in MeV). We will assume that the energies are randomly distributed about a common mean, and that each event is uncorrelated to others. Read the dataset from the enclosed file and:\n",
    "1. Produce a histogram of the distribution of energies. Choose the number of bins wisely, i.e. so that the width of each bin is smaller than the width of the peak, and at the same time so that the number of entries in the most populated bin is relatively large. Since this plot represents randomly-collected data, plotting error bars would be appropriate.\n",
    "1. Compute the mean and standard deviation of the distribution of energies and their statistical uncertainties. Assume the distribution is Gaussian and see the lecture notes for the formulas for the mean and variance of the sample and the formulas for the errors on these quantities. \n",
    "1. Fit the distribution to a Gaussian function using a binned least-squares fit (<i>Hint:</i> use <tt>scipy.optimize.curve_fit()</tt> function), and compare the parameters of the fitted Gaussian and their uncertainties to the parameters obtained in Part (2) \n",
    "1. Re-make your histogram from Part 1 with twice as many bins, and repeat the binned least-squares fit from Part 3 on the new histogram. How sensitive are your results to binning ? \n",
    "1. How consistent is the distribution with a Gaussian? In other words, compare the histogram from Part 1 to the fitted curve, and compute a goodness-of-fit value, such as $\\chi^2$/d.f.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import scipy.optimize as fitter\n",
    "\n",
    "\n",
    "# Once again, feel free to play around with the matplotlib parameters\n",
    "plt.rcParams['figure.figsize'] = 8,4\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "\n",
    "energies = np.loadtxt('peak.dat') # MeV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall `plt.hist()` isn't great when you need error bars, so it's better to first use [`np.histogram()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html) -- which returns the counts in each bin, along with the edges of the bins (there are $n + 1$ edges for $n$ bins).  Once you find the bin centers and errors on the counts, you can make the actual plot with [`plt.bar()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.bar.html).  Start with something close to `bins = 25` as the second input parameter to `np.histogram()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy.histogram to get the counts and bin edges\n",
    "\n",
    "# bin_centers = 0.5*(bin_edges[1:]+bin_edges[:-1]) works for finding the bin centers\n",
    "\n",
    "# assume Poisson errors on the counts â€“ errors go as the square root of the count\n",
    "\n",
    "# now use plt.bar() to make the histogram with error bars (remember to label the plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation of the list of `energies` and their uncertainties\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the binned values (found above with `np.histogram()`) and their errors use `scipy.optimize.curve_fit()` to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, curve_fit() will need a model function defined\n",
    "def model(x, A, mu, sigma):\n",
    "    '''Model function to use with curve_fit();\n",
    "       it should take the form of a 1-D Gaussian'''\n",
    "\n",
    "# Also make sure you define some starting parameters for curve_fit (we typically called these par0 or p0 in the past workshop)\n",
    "\n",
    "'''# You can use this to ensure the errors are greater than 0 to avoid division by 0 within fitter.curve_fit()\n",
    "for i, err in enumerate(counts_err):\n",
    "    if err == 0:\n",
    "        counts_err[i] = 1'''\n",
    "\n",
    "# Now use fitter.curve_fit() on the binned data and compare the best-fit parameters to those found by scipy.stats.norm.fit()\n",
    "# It's also useful to plot the fitted curve over the histogram you made in part 1 to check that things are working properly\n",
    "\n",
    "# At this point, it's also useful to find the chi^2 and reduced chi^2 value of this binned fit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this process with twice as many bins (i.e. now use `bins = 50` in `np.histogram()`, or a similar value).  Compute the $\\chi^2$ and reduced $\\chi^2$ and compare these values, along with the best-fit parameters between the two binned fits.  Feel free to continue to play with the number of bins and see how it changes the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the <i>goodness-of-fit</i> parameters, i.e. $\\chi^2$ and $\\chi^2/N_{df}$. Compute the quantile of the $\\chi^2$ for a standard `chi2` distribution with $N_{df}$ degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
